{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c3ffe06-c153-4775-aa6f-ddc81745ec89",
   "metadata": {},
   "source": [
    "# Lightning to PyTorch: CIFAR-10 Classifier Comparison\n",
    "\n",
    "This notebook is part of my prep for developing skills as a research engineer in alignment science role\n",
    "\n",
    "The goal is to deeply understand PyTorch Lightning abstractions by:\n",
    "- Annotating every major component of the Lightning CIFAR-10 example\n",
    "- Rebuilding the same pipeline in plan PyTorch side-by-side\n",
    "- Documenting what Lightning automates or hides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f41b6dc-a750-44a6-a468-c35ab6d6a57d",
   "metadata": {},
   "source": [
    "## Part 1: Vanilla PyTorch Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b28ef1b-2041-4d99-a7ca-151d42855dbc",
   "metadata": {},
   "source": [
    "## Part 2: Lightning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbc39c3-fbbe-41bc-a16a-2c0cea852ee4",
   "metadata": {},
   "source": [
    "### Step 1: Import Python and Pytorch Libraries\n",
    "\n",
    "We import standard Python Libraries, Pytorch core components, and torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5340a0d3-94f7-449e-a2f4-65a6b9bc3274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d3aba2-4058-482b-8026-b637fd950207",
   "metadata": {},
   "source": [
    "### Step 2: Import Functional Tools and Dataset Utilities\n",
    "\n",
    "We'll use functional tools like 'F.cross_entropy' (calculates classification loss in the training step of the model) and dataset helpers like 'transforms' (preprocesses images when loading image data to the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfbf270d-3921-4b18-a633-8c9858ccde2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5218375e-9187-40a4-a307-a7b0ff17720a",
   "metadata": {},
   "source": [
    "### Step 3: Import PyTorch Lightning Core Tools\n",
    "\n",
    "We import the core PyTorch Lightning classes for model training and reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90aa934a-9b57-4dc2-a177-e0287177ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b3f4e8-bfd8-4ac9-b397-09b8cc2e378d",
   "metadata": {},
   "source": [
    "### Step 4: Set a Seed for Reproducibility\n",
    "\n",
    "We fix the random seed to ensure experiments are reproducible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eca1a8de-e26f-4ac4-92ac-94190b15b0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2bd397-3e0d-4615-9899-1a47d507d93d",
   "metadata": {},
   "source": [
    "### Step 5: Define Transform for CIFAR-10\n",
    "\n",
    "CIFAR-10 images are 32x32 color images (3 channels, RGB). We apply:\n",
    "- 'ToTensor()' to conver PIL images to PyTorch tensors\n",
    "- 'Normalize()' to scale pixel values form [0,1] to [-1,1]\n",
    "This helps model learn better and faster.\n",
    "\n",
    "The below is a data transformation pipeline to proprocess the images.\n",
    "- transforms.ToTensor() converts image to tensor, scales to [0.0, 1.0] e.g. before: pixel value is 128 out of 255, after ToTensor: 128/255 = ~0.502\n",
    "- transforms.Normalize(...) shifts and scales to [-1,1] per channel (R, G, B) e.g. after Normalize (0.502 - 0.5) / 0.5 = ~ 0.004 / 0.5 = ~ 0.008: pixel value of 0 become - 1, a pixel value of 1 becomes + 1 so range is roughly [-1, 1]\n",
    "- transforms.Compose([...]) chains the two steps into one transformation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f675c9e2-5899-47c3-a21e-16cfc7028a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02095afd-f159-43f3-ab79-e18437c8db31",
   "metadata": {},
   "source": [
    "### Step 6: Download the CIFAR-10 Trainig Set\n",
    "\n",
    "We use 'torchvision.datasets.CIFAR10' to download the training portion of the dataset\n",
    "We apply the transform so that each image is automatically preprocessed when accessed. \n",
    "\n",
    "Notes:\n",
    "root='.' -> saves data in the current directory\n",
    "train=True -> selects the training set (train=False for test set)\n",
    "download=True -> downloads the dataset if not already present\n",
    "transform=transfrom -> applies the transform preprocessing pipeline to each image when it is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26eb89e6-9393-4878-8635-10333ab72cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cifar_train = CIFAR10(root='.', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef4918-4672-4165-be19-519ea2f47d3b",
   "metadata": {},
   "source": [
    "### Step 7: Split CIFAR-10 Training Set into Training and Validation Set\n",
    "\n",
    "We split the 50,000 training images into:\n",
    "- 45,0000 for training\n",
    "- 5,000 for validation\n",
    "\n",
    "So we can monitor how well the model is generalizing during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44e50192-aecc-43e5-a1d8-efc579d1d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_train, cifar_val = random_split(cifar_train, [45000, 5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1301ec7a-6e2d-44cc-8f11-d31351a6bd1a",
   "metadata": {},
   "source": [
    "### Step 8: Create DataLoaders for Training and Validation Sets\n",
    "\n",
    "We now create PyTorch DataLoaders to:\n",
    "- Shuffle and batch the training data\n",
    "- Batch (but not shuffle) the validation data\n",
    "\n",
    "Batch size controls how many images the model sees at once during training\n",
    "\n",
    "Notes:\n",
    "- for validation, we don't shuffle to ensure consistency\n",
    "- can't load 50,000 images into GPU memory at once so DataLoader gives small manageable chunks\n",
    "- Models learn better when trained on mini-batches rather than single images or full dataset\n",
    "- batch size of 64 images at a time is a common default as a good trade-off between speed and stability (less memory, sometimes better generalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ef6decf-1afc-45db-a634-05a2fe86e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(cifar_train, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(cifar_val, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f587314-faf0-4af2-9b35-84fbfc318f59",
   "metadata": {},
   "source": [
    "### Step 9: Define the CNN Model Architecture\n",
    "\n",
    "We define a simple Convolutional Neural Network (CNN) using Pytorch's 'nn.Module'. It has:\n",
    "- Two convolutional layers\n",
    "- ReLU activations and max pooling\n",
    "- Two fully connected (linear) layers\n",
    "Takes a 3 x 32 x 32 image and outputs 10 class scores (for CIFAR-10)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7bece2-82c7-4a6d-b708-bbc20c7adc33",
   "metadata": {},
   "source": [
    "#### Step 9.1: Import CNN and activation libraries\n",
    "\n",
    "We import the core building blocks to define neural networks.\n",
    "- `nn.Module` for model classes\n",
    "- `Conv2d`, `Linear`, etc. for layers\n",
    "- `F.relu()` for activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a59675-b449-48dd-9a63-0beef0794f8e",
   "metadata": {},
   "source": [
    "#### Step 9.2: Define the CNN Model Class\n",
    "\n",
    "We create a class `SimpleCNN` that extends `nn.Module`. This is how all PyTorch models are structured.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13ea501-9c83-4dc6-bcf3-0365ed3ecc11",
   "metadata": {},
   "source": [
    "#### Step 9.3: First Convolutional Layer\n",
    "\n",
    "This layer:\n",
    "- Takes RGB images (3 channels)\n",
    "- Applies 32 filters of size 3x3\n",
    "- Uses padding=1 to keep spatial dimensions the same\n",
    "\n",
    "Output: [B, 32, 32, 32]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3bde56-8ea4-4c44-b42e-9b1f4c0db735",
   "metadata": {},
   "source": [
    "#### Step 9.4: Max Pooling Layer\n",
    "\n",
    "Reduces each spatial dimension by half (32x32 -> 16x16) using a 2x2 max filter.\n",
    "This reduces computation and focuses on st"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320c40bf-8d5f-4d6f-a561-8164b2ca3e44",
   "metadata": {},
   "source": [
    "#### Step 9.5: Second Convolutional Layer\n",
    "\n",
    "Applies 64 filters to the output of the first pooled layer.\n",
    "\n",
    "Again uses padding=1 to preserve dimensions before pooling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23d42cc-6b63-46d0-a888-08cd134343a2",
   "metadata": {},
   "source": [
    "#### Step 9.6: First Fully Connected (Linear) Layer\n",
    "\n",
    "- After second pooling: output is [B, 64, 8, 8]\n",
    "- We flatten this to [B, 4096] before feeding into a dense layer\n",
    "\n",
    "This layer reduces 4096 features to 256.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f0613-0589-4f75-924a-eb804b6649db",
   "metadata": {},
   "source": [
    "#### Step 9.7: Output Layer\n",
    "\n",
    "This layer maps the 256 features to 10 output scores (one for each CIFAR-10 class).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbd88ec-0f9a-41e4-ba0e-9b5c27af9049",
   "metadata": {},
   "source": [
    "#### Step 9.8: Define the Forward Pass\n",
    "\n",
    "This method defines how the input flows through the network layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f8f91b-df28-4d18-a08a-0ea47eabfc21",
   "metadata": {},
   "source": [
    "##### Note: the entire class SimpleCNN must be one block so Python knows how to build the object correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d7d1d6b-7abd-4458-911f-386b8f8b31ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # First conv layer: in_channels=3 (RGB), out_channels=32, kernel=3x3\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Second conv layer\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Flattening and linear layers\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 256)  # 8x8 is the output size after 2 poolings\n",
    "        self.fc2 = nn.Linear(256, 10)  # 10 classes in CIFAR-10\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> [batch, 32, 16, 16]\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # -> [batch, 64, 8, 8]\n",
    "        x = x.view(-1, 64 * 8 * 8)            # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfe8422-2418-40b6-b8bc-a81741c8ba51",
   "metadata": {},
   "source": [
    "### Step 10: Run a Forward Pass with Dummpy Input\n",
    "\n",
    "We'll create a fake batch of CIFAR-10 images (3 channels, 32x32), pass it through the model, and check the output shape.\n",
    "\n",
    "Expected output: [batch_size, 10] - the logits for 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c71e6d0-0043-45ec-99b7-136f99b4239a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([8, 10])\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create a fake batch of images\n",
    "# Shape: [batch_size, channels, height, width]\n",
    "dummy_images = torch.randn(8, 3, 32, 32)  # batch of 8 RGB images\n",
    "\n",
    "# Step 2: Instantiate the model\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Step 3: Forward pass\n",
    "outputs = model(dummy_images)\n",
    "\n",
    "# Step 4: Print the output shape\n",
    "print(\"Output shape:\", outputs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cbbd55-e202-4b20-9f2a-c4a685ec5cb2",
   "metadata": {},
   "source": [
    "### Step 11: Setup Training Tools (Vanilla Pytorch)\n",
    "\n",
    "Training loop is the heart of deep learning. Basic idea:\n",
    "For each epoch (a full pass through dataset):\n",
    "1. Loop through mini-batches of your training data\n",
    "2. For each batch:\n",
    "   Run the model's forward pass (input -> model -> output; see how well the model is performing)\n",
    "   Compute the loss\n",
    "   Run backward pass to compute gradients (loss -> derivatives -> backpropagation; figure out how to adjust the weights)\n",
    "   Update weights using the optimizer\n",
    "3. Optionally evaluate on validation data\n",
    "\n",
    "Forward pass - throw the dart and see where it lands - how wrong am I?\n",
    "Backward pass - calculate how to adjust your arm to get closer next time\n",
    "\n",
    "We now define:\n",
    "- The **loss function**: Cross-entropy for classification\n",
    "- The **optimizer**: Adam optimizer for parameter updates\n",
    "- The **device**: Use GPU if available, otherwise CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "849cafd9-8286-4550-a281-234c0ee0dbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Step 2: Instantiate model and move it to device\n",
    "model = SimpleCNN().to(device)\n",
    "\n",
    "# Step 3: Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Step 4: Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c7c605-6a80-47e4-ab5f-64f845fa5f44",
   "metadata": {},
   "source": [
    "### Step 12: Training Loop (Vanilla PyTorch)\n",
    "\n",
    "We'll train the model for a few epochs using the training DataLoader.\n",
    "Each epoch consists of:\n",
    "1. Forward pass - to compute predictions\n",
    "2. Loss calculation - to measure error\n",
    "3. Backward pass - to compute gradients\n",
    "4. Optimizer step - to update weights\n",
    "\n",
    "We'll also print the avergae training loss per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf561696-7a9c-4108-a8af-f47139260e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Echo 1/5, Training Loss: 1.3235\n",
      "Echo 2/5, Training Loss: 0.9305\n",
      "Echo 3/5, Training Loss: 0.7597\n",
      "Echo 4/5, Training Loss: 0.6280\n",
      "Echo 5/5, Training Loss: 0.4971\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() # set model to training mode\n",
    "    running_loss = 0.0 \n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad() #1. Clear old gradients from the last step\n",
    "        outputs = model(inputs) #2. Forward pass\n",
    "        loss = criterion(outputs, labels) # 3. Compute loss\n",
    "        loss.backward() #4. Backward pass, computes graidents\n",
    "        optimizer.step() #5. Update weights using gradients\n",
    "\n",
    "        running_loss += loss.item() #Accumulate total loss for the epoch\n",
    "\n",
    "    avg_loss = running_loss/len(train_loader)\n",
    "    print(f\"Echo {epoch+1}/{num_epochs}, Training Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaf1033-0ede-4f39-9127-3111d101ecd2",
   "metadata": {},
   "source": [
    "### Step 14: Run Validation Separately After Training\n",
    "\n",
    "Once we've trained the model, we can evaluate it using the validation set in a separate cell.\n",
    "\n",
    "This step:\n",
    "- Switches the model to evaluation mode (`model.eval()`)\n",
    "- Turns off gradient tracking (`torch.no_grad()`)\n",
    "- Loops through the validation DataLoader\n",
    "- Computes average loss and classification accuracy\n",
    "\n",
    "This is useful for testing how well the model performs on unseen validation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb41c744-f64e-42d7-a09a-7e3a11a4f9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8281, Accuracy: 0.7254\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode to disable dropout/batchnorm updates\n",
    "model.eval()\n",
    "\n",
    "# Initialize counters for loss and accuracy\n",
    "val_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Disable gradient tracking to save memory and speed up computation\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        # Move data to the same device as the model (GPU or CPU)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass: compute model output\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        # Get the predicted class (index with highest score)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "        # Count correct predictions\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Compute average loss and overall accuracy\n",
    "avg_val_loss = val_loss / len(val_loader)\n",
    "val_accuracy = correct / total\n",
    "\n",
    "# Print the results\n",
    "print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773796ec-ec85-44ac-9693-85f3b974c5bc",
   "metadata": {},
   "source": [
    "### Step 15: Evaluate the Model on the CIFAR-10 Test Set\n",
    "\n",
    "The test set consists of 10,000 examples that were not used in training or validation.\n",
    "\n",
    "We:\n",
    "- Download the test dataset and apply the same transform\n",
    "- Create a DataLoader for batching\n",
    "- Evaluate the model’s final performance on this unseen data\n",
    "\n",
    "This is the best estimate of how well our model will generalize in the real world.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4ca8ceb-881f-4194-82f6-da78e5d10377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8423, Test Accuracy: 0.7286\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the CIFAR-10 test set\n",
    "cifar_test = CIFAR10(root='.', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(cifar_test, batch_size=64)\n",
    "\n",
    "# Step 2: Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Step 3: Initialize counters\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Step 4: Run inference\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Step 5: Compute final metrics\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "test_accuracy = correct / total\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f502b3fc-9299-41c9-ba17-850246a5d5b0",
   "metadata": {},
   "source": [
    "### Step 16: Save the Trained Model\n",
    "\n",
    "We save the model’s learned weights using `torch.save()`. \n",
    "This lets us reload the model later for inference or continued training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f501278-4c0c-4e82-9195-7bab539890ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to simple_cnn.pth\n"
     ]
    }
   ],
   "source": [
    "# Save model parameters to a file\n",
    "torch.save(model.state_dict(), \"simple_cnn.pth\")\n",
    "print(\"Model weights saved to simple_cnn.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c68b1e-54cf-46dc-a087-11cd6d0abce1",
   "metadata": {},
   "source": [
    "## Translation to PyTorch Lightning version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0d219a-4e51-4f8c-8131-c5bf157ae776",
   "metadata": {},
   "source": [
    "### Step 1: Import PyTorch Lightning\n",
    "\n",
    "We import the core Lightning module that lets us wrap our model in a training framework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2c95b76-9292-4634-884d-b721b20292aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1ad1e3-1093-48ce-88d7-6c839f115a6e",
   "metadata": {},
   "source": [
    "### Step 2: Define LightningModule to Wrap the CNN\n",
    "\n",
    "We create a `LitCNN` class that extends `LightningModule`.\n",
    "\n",
    "This class defines:\n",
    "- The model (`self.model`)\n",
    "- Training logic (`training_step`)\n",
    "- Validation logic (`validation_step`)\n",
    "- Optimizer (`configure_optimizers`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b574f-8f45-4a54-a080-b3dd0f7d4614",
   "metadata": {},
   "source": [
    "e create a new class `LitCNN` that inherits from `LightningModule`. This class wraps our original `SimpleCNN` model and handles:\n",
    "\n",
    "#### 🔧 Model Instantiation (`__init__`)\n",
    "- We create an instance of our original CNN (`SimpleCNN`) and assign it to `self.model`\n",
    "- We define a loss function (`CrossEntropyLoss`) to compare predicted vs. true labels\n",
    "\n",
    "#### 🔁 Forward Pass (`forward`)\n",
    "- This is a wrapper around `self.model(x)`\n",
    "- It's used when the Trainer or another function calls `model(x)` internally\n",
    "\n",
    "#### 📉 Training Step (`training_step`)\n",
    "- This method is called **once per batch** during training\n",
    "- It receives a batch of data: `x` (inputs) and `y` (true labels)\n",
    "- It runs the forward pass to get predictions (logits)\n",
    "- Computes the loss using the criterion\n",
    "- Logs the loss using `self.log()` so Lightning can track metrics\n",
    "\n",
    "#### ✅ Validation Step (`validation_step`)\n",
    "- Same idea as `training_step`, but used on the validation set\n",
    "- In addition to loss, we also calculate accuracy:\n",
    "  - We take the class with the highest logit (`argmax`)\n",
    "  - We compare it to the ground truth and average over the batch\n",
    "\n",
    "#### ⚙️ Optimizer Setup (`configure_optimizers`)\n",
    "- We define and return our optimizer here (Adam in this case)\n",
    "- Lightning will automatically call `optimizer.step()` for us each batch\n",
    "\n",
    "---\n",
    "\n",
    "**✨ Benefits of Lightning:**\n",
    "- We no longer have to manually write training loops, validation loops, or optimizer steps\n",
    "- All the common training infrastructure is handled by the `Trainer`\n",
    "- We can scale up (e.g., to multi-GPU) or log metrics easily without changing model code\n",
    "\n",
    "Once this class is defined, we’ll use `Trainer.fit()` to train it, passing in data and hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ccc9bab-45ff-4544-a74a-bb65743730a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitCNN(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = SimpleCNN()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):  # <-- THIS PART MUST BE INSIDE THE CLASS\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee50922-5965-4626-b899-7bd856b15fbc",
   "metadata": {},
   "source": [
    "### Step 3: Train the Lightning Model Using Trainer\n",
    "\n",
    "PyTorch Lightning’s `Trainer` class handles:\n",
    "- Training loop\n",
    "- Validation after each epoch\n",
    "- Progress bars and logging\n",
    "- GPU/CPU device management\n",
    "\n",
    "We just pass in:\n",
    "- Our `LitCNN` model\n",
    "- Number of training epochs\n",
    "- DataLoaders for training and validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff8d5758-6886-4bd1-a89c-c4c49f5c86be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Volumes/Stay-C SSD/AI Projects/torch-utils/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | SimpleCNN        | 1.1 M  | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.283     Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc38bbb06e844bebb3ce8dd77c69eb1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                     | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Stay-C SSD/AI Projects/torch-utils/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Volumes/Stay-C SSD/AI Projects/torch-utils/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22eb73d90fc545278be3cf58b254c74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                            | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "361743ddd9e84783bc3db985e7f90238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ba4e7a089e44a38fa1ef3d65470e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd563686ebd475999700042b4159ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cfad3a2f2104f71880cdc7587648aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7938f91a4e664c19b213fe66aff8352b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "# Step 1: Instantiate the Lightning-wrapped model\n",
    "lit_model = LitCNN()\n",
    "\n",
    "# Step 2: Create a Trainer (automatic GPU if available)\n",
    "trainer = Trainer(max_epochs=5, accelerator='auto')\n",
    "\n",
    "# Step 3: Train the model\n",
    "trainer.fit(lit_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09a08e7-66a0-46b5-abc0-3b022b27e1fd",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate the Trained Model on the Test Set\n",
    "\n",
    "Now that the model is trained, we’ll evaluate it on the official CIFAR-10 test set.\n",
    "We'll:\n",
    "- Load the test data with the same transform\n",
    "- Wrap it in a DataLoader\n",
    "- Use `trainer.test()` to run evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a2654f1-2038-46e4-b22c-97f5c7cd54aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Stay-C SSD/AI Projects/torch-utils/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddcf13c87a6146da977b4f7982412616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.7163000106811523\n",
      "        test_loss           0.8720601797103882\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.8720601797103882, 'test_acc': 0.7163000106811523}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=lit_model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca99ca2-b7d1-4c24-bd39-172a43cf134a",
   "metadata": {},
   "source": [
    "### Step 4: Save the Trained Lightning Model Weights\n",
    "\n",
    "We save only the model weights using `torch.save()`. \n",
    "To reload later, we’ll re-instantiate the `LitCNN` model and load these saved weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5c5db40-0e3e-476b-8777-28e8b5cd4063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to lit_cnn_weights.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model weights to disk\n",
    "torch.save(lit_model.state_dict(), \"lit_cnn_weights.pth\")\n",
    "print(\"Model weights saved to lit_cnn_weights.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb029c5-699a-4bf9-92cc-acc797d0fd7a",
   "metadata": {},
   "source": [
    "### Why I Didn't Use a LightningDataModule Initially — and When It's Useful\n",
    "\n",
    "In this notebook, I used **explicit PyTorch DataLoaders (`train_loader`, `val_loader`, `test_loader`)** rather than implementing a `LightningDataModule` from the start.\n",
    "\n",
    "This was intentional for the following reasons:\n",
    "\n",
    "#### ✅ 1. Focus on Learning Core Concepts\n",
    "For this first round of experimentation, my primary goal was to:\n",
    "- Understand how PyTorch Lightning abstracts away training loops\n",
    "- Learn how to implement a model, training, validation, and testing steps manually\n",
    "- See the full lifecycle of a model in Lightning from scratch\n",
    "\n",
    "Using raw `DataLoader`s helped me see **exactly what Lightning is doing**, without the added abstraction of a separate data module.\n",
    "\n",
    "#### ✅ 2. Faster Iteration While Prototyping\n",
    "When you're:\n",
    "- Trying out different batch sizes\n",
    "- Quickly switching between datasets\n",
    "- Experimenting with transforms\n",
    "...it's often faster to work directly with `DataLoader`s and tweak inline.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ When to Use a LightningDataModule (and Why It's Powerful)\n",
    "\n",
    "As projects grow in complexity, `LightningDataModule` becomes extremely useful:\n",
    "\n",
    "#### ✅ Cleaner Organization\n",
    "All data loading logic — downloading, splitting, transforming — lives in one reusable class. This avoids copy-pasting DataLoader code between notebooks or scripts.\n",
    "\n",
    "#### ✅ Reusability\n",
    "You can plug the same `DataModule` into different models or experiments — making your codebase modular and scalable.\n",
    "\n",
    "#### ✅ Production Readiness\n",
    "If you're deploying a model or running long training jobs:\n",
    "- `prepare_data()` only downloads once (on one process)\n",
    "- `setup()` runs at the right time per training/validation/test phase\n",
    "- Easy to switch between CPU, GPU, or distributed systems\n",
    "\n",
    "#### ✅ Compatibility with `Trainer.predict()`, `Trainer.test()`, CLI training, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Summary\n",
    "\n",
    "Using explicit DataLoaders helped me:\n",
    "- Learn PyTorch Lightning’s core logic\n",
    "- Stay hands-on with each step\n",
    "- Build confidence in manual training loop concepts\n",
    "\n",
    "But for cleaner long-term experiments, I’ve now transitioned to a `CIFAR10DataModule` for better structure and scalability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a321de-8cfe-4952-9482-ef762a68bd3f",
   "metadata": {},
   "source": [
    "### Why Performance Improvement Strategies Are Important for Future Development\n",
    "\n",
    "Once a model is successfully training and evaluating, the next step is to explore ways to **improve performance**, especially generalization to unseen data.\n",
    "\n",
    "Neural networks can often achieve **much higher accuracy** and robustness with a few thoughtful upgrades to data, architecture, or training setup.\n",
    "\n",
    "Below are common strategies that are broadly applicable in deep learning workflows:\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔁 1. Training for More Epochs\n",
    "- Training for a small number of epochs is great for prototyping.\n",
    "- For better performance, more epochs can allow the model to learn richer patterns.\n",
    "- Always monitor validation loss to avoid overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🖼️ 2. Data Augmentation\n",
    "- Applying random transformations (flips, crops, noise) makes training data more diverse.\n",
    "- Improves the model’s ability to generalize beyond what it has seen.\n",
    "- Especially effective on small or fixed-size datasets like CIFAR-10.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧠 3. Using a Deeper or Pretrained Architecture\n",
    "- Simple models are great for learning, but deeper architectures (like ResNet, VGG, etc.) often perform much better.\n",
    "- Pretrained models from `torchvision.models` can dramatically improve results with minimal setup.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🎛️ 4. Hyperparameter Tuning\n",
    "- Learning rate, optimizer choice, and batch size can have a huge impact on model convergence and stability.\n",
    "- Systematic tuning or using libraries like `Optuna` can help discover better combinations.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧪 5. Regularization Techniques\n",
    "- Dropout, weight decay (L2), or label smoothing help prevent overfitting.\n",
    "- These methods are often easy to add and can significantly improve robustness.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧼 6. Early Stopping and Model Checkpointing\n",
    "- Automatically save the best version of the model during training.\n",
    "- Stop training if validation accuracy stops improving.\n",
    "- Saves time and protects against overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Summary\n",
    "\n",
    "These strategies offer a roadmap for improving model performance once the training pipeline is stable. They help:\n",
    "- Boost accuracy\n",
    "- Improve generalization\n",
    "- Support robust, scalable model development\n",
    "\n",
    "As experimentation continues, combining several of these techniques thoughtfully can make a significant difference in results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3f2730-91e1-4ff1-a87c-254a00831a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
